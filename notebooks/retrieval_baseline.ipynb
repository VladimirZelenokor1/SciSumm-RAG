{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieval Baseline Notebook",
   "id": "492bb8d52e8239c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieval Baseline for SciSumm-RAG\n",
    "\n",
    "In this notebook we:\n",
    "- Load the FAISS index and chunks (chunks.jsonl)\n",
    "- Demonstrate retrieval (direct and hybrid)\n",
    "- Generate summary via HFSummarizer\n",
    "- Evaluate retrieval quality qualitatively and quantitatively (ROUGE)"
   ],
   "id": "8b83413085e886fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:14:17.438715Z",
     "start_time": "2025-07-11T22:14:17.434887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "import sys\n",
    "from pathlib import Path"
   ],
   "id": "b9a8a7cc51ad87b5",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:14:19.369708Z",
     "start_time": "2025-07-11T22:14:19.362831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auto-detection project_root: go up until we find data/clean/embeddings.npy\n",
    "root = Path.cwd()\n",
    "while not (root / \"data\" / \"clean\" / \"embeddings.npy\").exists():\n",
    "    # if we get to the root of the file system - exit with an error\n",
    "    if root.parent == root:\n",
    "        raise RuntimeError(\"Could not find the folder data/clean/embeddings.npy\")\n",
    "    root = root.parent\n",
    "\n",
    "project_root = root\n",
    "print(\"project_root:\", project_root)\n",
    "\n",
    "# Paths\n",
    "clean_dir = project_root / \"data\" / \"clean\"\n",
    "index_dir = project_root / \"data\" / \"index\" / \"faiss\"\n",
    "\n",
    "emb_path = clean_dir / \"embeddings.npy\"\n",
    "ids_path = clean_dir / \"ids.json\"\n",
    "\n",
    "flat_index_path   = index_dir / \"flat_index.index\"\n",
    "flat_ids_path     = index_dir / \"flat_index_ids.json\"\n",
    "ivfopq_index_path = index_dir / \"ivfpq_index.index\"\n",
    "ivfopq_ids_path   = index_dir / \"ivfpq_index_ids.json\"\n",
    "\n",
    "# To make importing src/... works\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"embeddings exists:\", emb_path.exists(), emb_path)\n",
    "print(\"ids exists:       \", ids_path.exists(), ids_path)"
   ],
   "id": "357b65cec2fb8355",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: D:\\SciSumm-RAG\n",
      "project_root: D:\\SciSumm-RAG\n",
      "embeddings exists: True D:\\SciSumm-RAG\\data\\clean\\embeddings.npy\n",
      "ids exists:        True D:\\SciSumm-RAG\\data\\clean\\ids.json\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:14:25.177340Z",
     "start_time": "2025-07-11T22:14:25.102325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.retriever.embed import embed_texts\n",
    "from src.retriever.index import (\n",
    "    normalize_embeddings,\n",
    "    search,\n",
    "    hybrid_search,\n",
    "    load_embeddings\n",
    ")\n",
    "from src.generator.hf_summarizer import HFSummarizer"
   ],
   "id": "13e0363aed0d7848",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HFSummarizer' from 'src.generator.hf_summarizer' (D:\\SciSumm-RAG\\src\\generator\\hf_summarizer.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mretriever\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membed\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m embed_texts\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mretriever\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mindex\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      3\u001B[39m     normalize_embeddings,\n\u001B[32m      4\u001B[39m     search,\n\u001B[32m      5\u001B[39m     hybrid_search,\n\u001B[32m      6\u001B[39m     load_embeddings\n\u001B[32m      7\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgenerator\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mhf_summarizer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HFSummarizer\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'HFSummarizer' from 'src.generator.hf_summarizer' (D:\\SciSumm-RAG\\src\\generator\\hf_summarizer.py)"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T20:52:34.171199Z",
     "start_time": "2025-07-11T20:52:12.929191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For ROUGE metrics\n",
    "!pip install rouge-score\n",
    "from rouge_score import rouge_scorer"
   ],
   "id": "b618211b2bf906ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in d:\\scisumm-rag\\venv\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in d:\\scisumm-rag\\venv\\lib\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\scisumm-rag\\venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\scisumm-rag\\venv\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\scisumm-rag\\venv\\lib\\site-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\scisumm-rag\\venv\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\scisumm-rag\\venv\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\scisumm-rag\\venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=35dd085fc922a119f3a5e4915359b325426eabd9875c16170ca1a741e63dabf1\n",
      "  Stored in directory: c:\\users\\zelen\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.3.1 rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T21:06:14.982364Z",
     "start_time": "2025-07-11T21:06:14.978020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_index_and_ids(\n",
    "    index_file: Path,\n",
    "    ids_file: Path\n",
    ") -> Tuple[faiss.Index, List[Tuple[str,str,str]]]:\n",
    "    # 1) читаем FAISS-индекс из .index-файла\n",
    "    idx = faiss.read_index(str(index_file))\n",
    "    # 2) читаем метаданные из *_ids.json\n",
    "    with open(ids_file, 'r', encoding='utf-8') as f:\n",
    "        raw = json.load(f)\n",
    "    # JSON хранит списки, приведём их к кортежам\n",
    "    ids = [tuple(x) for x in raw]\n",
    "    return idx, ids"
   ],
   "id": "691e46df8471adc8",
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-11T21:06:16.615828Z",
     "start_time": "2025-07-11T21:06:16.608670Z"
    }
   },
   "source": [
    "ids, vecs = load_embeddings(emb_path, ids_path)\n",
    "# turn each [paper_id, section, chunk_id] into a tuple\n",
    "ids = [tuple(x) for x in ids]\n",
    "vecs = vecs.astype('float32')  \n",
    "\n",
    "index_flat, ids_flat = load_index_and_ids(flat_index_path, flat_ids_path)"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T21:06:18.659575Z",
     "start_time": "2025-07-11T21:06:18.652725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Loading chunk_texts mapping from JSONL file\n",
    "chunks_file = clean_dir / \"chunks.jsonl\" \n",
    "chunk_texts = {}\n",
    "with open(chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pid, section, cid, txt = json.loads(line)\n",
    "        chunk_texts[(pid, section, cid)] = txt"
   ],
   "id": "3dc2066680b07fd0",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Preparation of the summarizer",
   "id": "a3292f1b3f888920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Используем HF summarizer\n",
    "summ = HFSummarizer()"
   ],
   "id": "e152117cb9ad3a6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T18:27:52.447144Z",
     "start_time": "2025-07-11T18:27:47.912235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = [\n",
    "    \"What is a mechanism for generating notebook interfaces for DSLs?\",  # CV/PL\n",
    "    \"How to stabilize corium during severe nuclear accident?\",           # Nuclear\n",
    "    \"What methods exist for probabilistic verification of software?\"   # ML/verification\n",
    "]\n",
    "\n",
    "# Embed & normalize все запросы\n",
    "q_embs = embed_texts(queries)\n",
    "q_embs = normalize_embeddings(q_embs.astype(np.float32))\n",
    "\n",
    "# выбираем тип retrieval: direct или hybrid\n",
    "use_hybrid = True\n",
    "results = []\n",
    "for q, q_emb in zip(queries, q_embs):\n",
    "    if use_hybrid:\n",
    "        res = hybrid_search(\n",
    "            coarse_idx=idx,\n",
    "            ids=ids,\n",
    "            queries=q_emb[np.newaxis, :],\n",
    "            query_texts=[q],\n",
    "            chunks_path=CHUNKS_PATH,\n",
    "            rerank_model=summ,  # или gpt\n",
    "            top_k_coarse=50,\n",
    "            top_k=5\n",
    "        )[0]\n",
    "    else:\n",
    "        dist, inds = idx.search(q_emb[np.newaxis,:], 5)\n",
    "        res = [(ids[i], float(dist_val)) for i, dist_val in zip(inds[0], dist[0])]\n",
    "    results.append(res)\n",
    "\n",
    "# Отобразим для первого запроса\n",
    "for rank, (key, score) in enumerate(results[0], start=1):\n",
    "    print(f\"{rank}. {key} (score={score:.3f})\")\n",
    "    print(chunk_texts[key][:200], \"...\\n\")"
   ],
   "id": "30bd4a12699b12a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We establish weak well - posedness for critical symmetric stable driven sdes in r d with additive noise z, d 1 . We study the case where the stable index of the driving process z is = 1 which exactly corresponds to the order of the drift term having the coefficient b which is continuous and bounded .\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for q, res in zip(queries, results):\n",
    "    print(\"\\nQUERY:\", q)\n",
    "    passages = [chunk_texts[k] for k,_ in res]\n",
    "    combined = \"\\n\\n\".join(passages)\n",
    "    summary = summ.summarize(combined, max_length=150, min_length=30)\n",
    "    print(\"SUMMARY:\", summary)"
   ],
   "id": "8112972c5b41f524"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:13:20.438192Z",
     "start_time": "2025-07-11T22:13:20.339139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# measure recall@k on a subsample (test_faiss_search.py script)\n",
    "!python test_faiss_search.py --embeddings data/clean/embeddings.npy --ids        data/clean/ids.json --index      data/index/faiss/flat_index.index --mode       flat --topk       5 --sample-size 1000      "
   ],
   "id": "489e485565acb98b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zelen\\AppData\\Local\\Programs\\Python\\Python312\\python.exe: can't open file 'D:\\\\SciSumm-RAG\\\\notebooks\\\\test_faiss_search.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b71096107f1bffe7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
