{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieval Baseline Notebook",
   "id": "492bb8d52e8239c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieval Baseline for SciSumm-RAG\n",
    "\n",
    "In this notebook we:\n",
    "- Load the FAISS index and chunks (chunks.jsonl)\n",
    "- Demonstrate retrieval (direct and hybrid)\n",
    "- Generate summary"
   ],
   "id": "8b83413085e886fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:06.199976Z",
     "start_time": "2025-07-12T10:31:06.182987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(root))\n",
    "sys.path.insert(0, str(root / \"src\"))"
   ],
   "id": "2e8977a65662d28d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:13.598790Z",
     "start_time": "2025-07-12T10:31:07.678001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import faiss, json, numpy as np\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ],
   "id": "b9a8a7cc51ad87b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SciSumm-RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:28.240592Z",
     "start_time": "2025-07-12T10:31:15.687163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.retriever.embed import embed_texts\n",
    "from src.retriever.index import (\n",
    "    normalize_embeddings,\n",
    "    search,\n",
    "    hybrid_search,\n",
    "    load_embeddings\n",
    ")\n",
    "from src.generator.hf_summarizer import HFSummarizer"
   ],
   "id": "b8011f42782d8c6e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:30.230107Z",
     "start_time": "2025-07-12T10:31:30.222987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auto-detection project_root: go up until we find data/clean/embeddings.npy\n",
    "root = Path.cwd()\n",
    "while not (root / \"data\" / \"clean\" / \"embeddings.npy\").exists():\n",
    "    # if we get to the root of the file system - exit with an error\n",
    "    if root.parent == root:\n",
    "        raise RuntimeError(\"Could not find the folder data/clean/embeddings.npy\")\n",
    "    root = root.parent\n",
    "\n",
    "project_root = root\n",
    "print(\"project_root:\", project_root)\n",
    "\n",
    "# Paths\n",
    "clean_data = project_root / \"data\" / \"clean\" / \"metadata_clean.csv\"\n",
    "clean_dir = project_root / \"data\" / \"clean\"\n",
    "index_dir = project_root / \"data\" / \"index\" / \"faiss\"\n",
    "\n",
    "emb_path = clean_dir / \"embeddings.npy\"\n",
    "ids_path = clean_dir / \"ids.json\"\n",
    "\n",
    "flat_index_path   = index_dir / \"flat_index.index\"\n",
    "flat_ids_path     = index_dir / \"flat_index_ids.json\"\n",
    "hnsw_index_path   = index_dir / \"hnsw_index.index\"\n",
    "hnsw_ids_path     = index_dir / \"hnsw_index_ids.json\"\n",
    "ivfpq_index_path = index_dir / \"ivfpq_index.index\"\n",
    "ivfpq_ids_path   = index_dir / \"ivfpq_index_ids.json\"\n",
    "opqivfpq_index_path = index_dir / \"opqivfpq_index.index\"\n",
    "opqivfpq_ids_path   = index_dir / \"opqivfpq_index_ids.json\"\n",
    "\n",
    "# To make importing src/... works\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"embeddings exists:\", emb_path.exists(), emb_path)\n",
    "print(\"ids exists:       \", ids_path.exists(), ids_path)"
   ],
   "id": "357b65cec2fb8355",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: D:\\SciSumm-RAG\n",
      "project_root: D:\\SciSumm-RAG\n",
      "embeddings exists: True D:\\SciSumm-RAG\\data\\clean\\embeddings.npy\n",
      "ids exists:        True D:\\SciSumm-RAG\\data\\clean\\ids.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:36.040801Z",
     "start_time": "2025-07-12T10:31:36.033769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_index_and_ids(\n",
    "    index_file: Path,\n",
    "    ids_file: Path\n",
    ") -> Tuple[faiss.Index, List[Tuple[str,str,str]]]:\n",
    "    # 1) read the FAISS index from the .index file\n",
    "    idx = faiss.read_index(str(index_file))\n",
    "    # 2) read metadata from *_ids.json\n",
    "    with open(ids_file, 'r', encoding='utf-8') as f:\n",
    "        raw = json.load(f)\n",
    "    # JSON stores lists, let's convert them to tuples\n",
    "    ids = [tuple(x) for x in raw]\n",
    "    return idx, ids"
   ],
   "id": "691e46df8471adc8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:37.875266Z",
     "start_time": "2025-07-12T10:31:37.864117Z"
    }
   },
   "source": [
    "ids, vecs = load_embeddings(emb_path, ids_path)\n",
    "# turn each [paper_id, section, chunk_id] into a tuple\n",
    "ids = [tuple(x) for x in ids]\n",
    "vecs = vecs.astype('float32')  \n",
    "\n",
    "index_flat, ids_flat = load_index_and_ids(flat_index_path, flat_ids_path)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:39.533457Z",
     "start_time": "2025-07-12T10:31:39.515783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Loading chunk_texts mapping from JSONL file\n",
    "chunks_file = clean_dir / \"chunks.jsonl\" \n",
    "chunk_texts = {}\n",
    "with open(chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        pid, section, cid, txt = json.loads(line)\n",
    "        chunk_texts[(pid, section, cid)] = txt"
   ],
   "id": "3dc2066680b07fd0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation of the summarizer",
   "id": "a3292f1b3f888920"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:31:44.611003Z",
     "start_time": "2025-07-12T10:31:44.599974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch \n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "cba133f082ec542d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:02.528384Z",
     "start_time": "2025-07-12T10:31:45.880881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HF summarizer\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cuda\")\n",
    "summarizer = HFSummarizer()"
   ],
   "id": "e152117cb9ad3a6c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:11.044740Z",
     "start_time": "2025-07-12T10:32:07.049534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = [\n",
    "    \"What is a mechanism for generating notebook interfaces for DSLs?\",  # CV/PL\n",
    "    \"How to stabilize corium during severe nuclear accident?\",           # Nuclear\n",
    "    \"What methods exist for probabilistic verification of software?\"   # ML/verification\n",
    "]\n",
    "\n",
    "# Embed & normalize all requests\n",
    "q_embs = embed_texts(queries)\n",
    "q_embs = normalize_embeddings(q_embs.astype(np.float32))"
   ],
   "id": "ce499d96567e2921",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:32:07,063 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:12.908004Z",
     "start_time": "2025-07-12T10:32:12.902081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# True — (dense+rerank), False — FAISS\n",
    "use_hybrid = True"
   ],
   "id": "b5db1de6d383c6e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:15.405425Z",
     "start_time": "2025-07-12T10:32:14.664583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "chunk_keys = list(chunk_texts.keys())\n",
    "\n",
    "for q, q_emb in zip(queries, q_embs):\n",
    "    if use_hybrid:\n",
    "        res = hybrid_search(\n",
    "            coarse_idx    = index_flat,\n",
    "            ids           = chunk_keys,\n",
    "            queries       = q_emb[np.newaxis, :],\n",
    "            query_texts   = [q],\n",
    "            chunks_path   = clean_dir / \"chunks.jsonl\",\n",
    "            rerank_model  = reranker,\n",
    "            top_k_coarse  = 100,\n",
    "            top_k         = 5\n",
    "        )[0]\n",
    "    else:\n",
    "        # FAISS search\n",
    "        distances, indices = index_flat.search(q_emb[np.newaxis, :], 5)\n",
    "        # (id, score)\n",
    "        res = [\n",
    "            (chunk_keys[i], float(score))\n",
    "            for i, score in zip(indices[0], distances[0])\n",
    "        ]\n",
    "    results.append(res)"
   ],
   "id": "30bd4a12699b12a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  9.79it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 29.18it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 26.93it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:16.777768Z",
     "start_time": "2025-07-12T10:32:16.768788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for q, res in zip(queries, results):\n",
    "    print(f\"\\nQUERY: {q}\")\n",
    "    for rank, (key, score) in enumerate(res, start=1):\n",
    "        pid, section, cid = key\n",
    "        snippet = chunk_texts[key][:200].replace(\"\\n\", \" \")\n",
    "        print(f\"{rank}. {pid} | {section} | {cid} (score={score:.3f})\")\n",
    "        print(\"   \", snippet, \"…\")"
   ],
   "id": "8112972c5b41f524",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY: What is a mechanism for generating notebook interfaces for DSLs?\n",
      "1. 2002.06180 | para_0 | para_0__1 (score=8.897)\n",
      "    interaction between end - users and dsls. approach : in this paper, we present bacat \\ ' a, a mechanism for generating notebook interfaces for dsls in a language parametric fashion. we designed this m …\n",
      "2. 2002.06180 | para_0 | para_0__0 (score=4.598)\n",
      "    context : computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single …\n",
      "3. 2002.06180 | para_0 | para_0__2 (score=2.609)\n",
      "    ), sweeterjs ( an extended version of javascript ), and ql ( a dsl for questionnaires ). additionally, it is relevant to generate notebook implementations rather than implementing them manually. we me …\n",
      "4. 2005.09028 | para_0 | para_0__1 (score=-2.127)\n",
      "    ranging from krishnamurthi ' s classic automata dsl to a sound synthesis dsl and a probabilistic programming language. all of these are existing dsls where we replaced the backend using sham, resultin …\n",
      "5. 2005.09028 | para_0 | para_0__0 (score=-2.647)\n",
      "    domain - specific languages ( dsls ) are touted as both easy to embed in programs and easy to optimize. yet these goals are often in tension. embedded or internal dsls fit naturally with a host langua …\n",
      "\n",
      "QUERY: How to stabilize corium during severe nuclear accident?\n",
      "1. 2408.15290 | para_0 | para_0__0 (score=7.850)\n",
      "    in - vessel retention ( ivr ) strategy for nuclear reactors in case of a severe accident ( sa ) intends to stabilize and retain the corium in the vessel by using the vessel wall as a heat exchanger wi …\n",
      "2. 2408.14522 | para_0 | para_0__0 (score=6.463)\n",
      "    molten corium stabilization following a severe accident is of crucial importance in order to ensure containment integrity on a long - term basis and minimizing radioactive elements releases outside th …\n",
      "3. 2408.15290 | para_0 | para_0__1 (score=-5.398)\n",
      "    of thermochemical interactions in the pool : when liquid steel is mixed with uo2 and partially oxidized zr coming from the degradation of the fuel and claddings, there is a phase separation between ox …\n",
      "4. 2408.14522 | para_0 | para_0__2 (score=-6.037)\n",
      "    define a common methodology to analyse ivr severe accident management ( sam ) strategy for the different types of eu npps. it started by reviewing the status of existing methodology and aimed at elabo …\n",
      "5. 2408.15290 | para_0 | para_0__2 (score=-9.843)\n",
      "    flux to the vessel wall and the investigation of possible means to avoid them. in this perspective, the calculations of ivr strategy done by the project partners for different reactor designs and acci …\n",
      "\n",
      "QUERY: What methods exist for probabilistic verification of software?\n",
      "1. 2304.13519 | para_0 | para_0__0 (score=-1.608)\n",
      "    due to increasing numbers of product piracy worldwide, a cost - effective method for verifying the origin of a product is to be developed. for this purpose, a certificate of authenticity can be create …\n",
      "2. 2306.12411 | para_0 | para_0__0 (score=-1.868)\n",
      "    a compiler consists of a sequence of phases going from lexical analysis to code generation. ideally, the formal verification of a compiler should include the formal verification of each component of t …\n",
      "3. 2111.10414 | para_0 | para_0__0 (score=-4.160)\n",
      "    context : this work is based on property - based testing ( pbt ). pbt is an increasingly important form of software testing. furthermore, it serves as a concrete gateway into the abstract area of form …\n",
      "4. 2206.14606 | para_0 | para_0__2 (score=-4.664)\n",
      "    work has been deployed in production two years ago, giving us insight on its actual use at scale every day. the git checkout authentication at its core is applicable beyond the specific use case of gu …\n",
      "5. 2203.02461 | para_0 | para_0__0 (score=-5.422)\n",
      "    in programming, protocols are everywhere. protocols describe the pattern of interaction ( or communication ) between software systems, for example, between a user - space program and the kernel or bet …\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T10:32:23.788129Z",
     "start_time": "2025-07-12T10:32:21.095832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for q, res in zip(queries, results):\n",
    "    top_pid = res[0][0]    # tuple (paper_id, section, cid)\n",
    "    pid     = top_pid[0]   # paper_id itself\n",
    "\n",
    "    # collect all the chunks of this article from the top 5\n",
    "    passages = [\n",
    "        chunk_texts[key]\n",
    "        for key, _ in res\n",
    "        if key[0] == pid\n",
    "    ]\n",
    "    combined = \"\\n\\n\".join(passages)\n",
    "\n",
    "    summary = summarizer.summarize(\n",
    "        combined,\n",
    "        max_length=150,\n",
    "        min_length=30\n",
    "    )\n",
    "    print(f\"\\nSUMMARY for {pid}:\\n\", summary)"
   ],
   "id": "4efa0998869585a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY for 2002.06180:\n",
      " bacat \\ ' a is a mechanism for generating notebook interfaces for dsls in a language parametric fashion . The tool can be used to generate notebooks for halide, sweeterjs, and ql for questionnaires . It can be easily generated with little manual configuration .\n",
      "\n",
      "SUMMARY for 2408.15290:\n",
      " in - vessel retention ( ivr ) strategy for nuclear reactors in case of a severe accident ( sa ) intends to stabilize and retain the corium in the vessel by using the vessel wall as a heat exchanger with an external water loop . This strategy relies on simple actions to be passively taken as soon as sa signal is raised : vessel depressurization and reactor pit flooding .\n",
      "\n",
      "SUMMARY for 2304.13519:\n",
      " A counterfeit - proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material . The characteristic positioning of the label ' s elements can be precisely measured using a smartphone ' s camera and additional technologies .\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
